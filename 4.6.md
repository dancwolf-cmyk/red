## 4.6 Results on Task 1: Addition (De-duplicated)

Task 1 evaluates natural-language addition queries whose outputs fall into five
explicit numerical ranges ($R_1$–$R_5$). Because these ranges are discretely
defined, the task enables clear comparison of global accuracy, range-wise
specialization, and routing stability. This section reports the results without
repeating architectural details.

---

### 4.6.1 Overall Performance

| Method                  | Accuracy | Macro-F1 |
|-|-|-|
| Global Head (base)      | 0.5503   | 0.5194   |
| Uniform Avg (all_fixed) | 0.2686   | 0.2295   |
| MoE Gating (moe)        | 0.8576   | 0.8568   |
| **RED (Ours, router)**  | **0.9725** | **0.9722** |

Macro-F1 is suppressed because each numerical range contains 99 target classes,
many with very small support. Thus, even a small number of rare-class errors
reduces the averaged F1 score substantially.

**Interpretation:**  
RED achieves the highest overall performance, improving upon MoE by approximately
$+11.5\%$ in accuracy, and substantially surpassing non-modular baselines.

---

### 4.6.2 Accuracy by Numerical Range ($R_1$–$R_5$)

The numerical ranges are:

- **$R_1$**: 2–20  
- **$R_2$**: 21–40  
- **$R_3$**: 41–60  
- **$R_4$**: 61–80  
- **$R_5$**: 81–100  

We categorize accuracy into three qualitative levels:

- **high:** ≥ 0.80  
- **moderate:** 0.55–0.80  
- **low:** < 0.55  

#### Raw Accuracy Results

- **Global Head (base)**  
  - $R_1$ 0.8994  
  - $R_2$ 0.4480  
  - $R_3$ 0.3131  
  - $R_4$ 0.3975  
  - $R_5$ 0.6719  

- **Uniform Avg (all_fixed)**  
  - $R_1$ 0.8978  
  - $R_2$ 0.0054  
  - $R_3$ 0.0000  
  - $R_4$ 0.0000  
  - $R_5$ 0.4104  

- **MoE Gating (moe)**  
  - $R_1$ 1.0000  
  - $R_2$ 0.7883  
  - $R_3$ 0.6836  
  - $R_4$ 0.7989  
  - $R_5$ 0.9907  

- **RED (router)**  
  - $R_1$ 1.0000  
  - $R_2$ 0.9822  
  - $R_3$ 0.9227  
  - $R_4$ 0.9527  
  - $R_5$ 1.0000  

#### Accuracy-Level Summary

| Method | $R_1$ | $R_2$ | $R_3$ | $R_4$ | $R_5$ |
|--|-|-|-|-|-|
| Global Head | high | low | low | low | moderate |
| Uniform Avg | high | low | low | low | low |
| MoE Gating | high | moderate | moderate | moderate | high |
| **RED (Ours)** | **high** | **high** | **high** | **high** | **high** |

---

### 4.6.3 Macro-F1 by Numerical Range

Macro-F1 averages the F1 score over the 99 output classes in each range. This
metric penalizes rare-class errors sharply, which explains why near-perfect
accuracy in $R_1$ and $R_5$ still corresponds to Macro-F1 around 0.19–0.20.

#### Raw Macro-F1 Results

- **Global Head (base)**  
  - $R_1$ 0.1706  
  - $R_2$ 0.0882  
  - $R_3$ 0.0627  
  - $R_4$ 0.0765  
  - $R_5$ 0.1304  

- **Uniform Avg (all_fixed)**  
  - $R_1$ 0.1826  
  - $R_2$ 0.0016  
  - $R_3$ 0.0000  
  - $R_4$ 0.0000  
  - $R_5$ 0.0950  

- **MoE Gating (moe)**  
  - $R_1$ 0.1919  
  - $R_2$ 0.1628  
  - $R_3$ 0.1379  
  - $R_4$ 0.1645  
  - $R_5$ 0.2001  

- **RED (router)**  
  - $R_1$ 0.1919  
  - $R_2$ 0.1984  
  - $R_3$ 0.1870  
  - $R_4$ 0.1926  
  - $R_5$ 0.2020  

#### Macro-F1 Levels

- **high:** ≥ 0.16  
- **moderate:** 0.08–0.16  
- **low:** < 0.08  

#### Macro-F1-Level Summary

| Method | $R_1$ | $R_2$ | $R_3$ | $R_4$ | $R_5$ |
|--|-|-|-|-|-|
| Global Head | moderate | low | low | low | moderate |
| Uniform Avg | moderate | low | low | low | low |
| MoE Gating | high | high | moderate | high | high |
| **RED (Ours)** | **high** | **high** | **high** | **high** | **high** |

---

### 4.6.4 Router Reliability

| Split | Router Accuracy |
|--|-|
| Train | 0.9714 |
| Validation | 0.9714 |
| Test | 0.9725 |

The router consistently assigns correct numerical ranges for approximately
97% of all samples. Because RED uses residual expert cooperation rather than
hard isolation, occasional routing errors do not destabilize predictions.

---

### 4.6.5 Behavior Near Range Boundaries

Boundary cases (e.g., 20/21, 40/41, 60/61, 80/81) are typically difficult for
modular systems.

Observed patterns:

- **Global Head:** biased toward more frequent mid-range outputs.  
- **Uniform Avg:** averaging destroys structure and leads to systematic drift.  
- **MoE:** predictions oscillate due to high routing sensitivity.  
- **RED:** smooth transitions and stable outputs, as residual connections allow
  neighboring experts to contribute.

Boundary robustness is one of RED’s distinguishing advantages compared with
classical MoE.

---

### 4.6.6 Summary of Task 1 Findings

Task 1 demonstrates that:

1. **RED achieves state-of-the-art accuracy**, outperforming MoE by a notable margin.  
2. **RED is the only method with uniformly high accuracy and Macro-F1 across all ranges.**  
3. **Router accuracy is consistently high**, and RED remains stable even under routing errors.  
4. **Expert interactions are smooth**, avoiding the volatility observed in MoE.

Overall, Task 1 provides strong evidence that RED supports reliable modular
reasoning in controlled arithmetic settings.

