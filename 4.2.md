## 4.2 Dataset Construction

This section presents the datasets used to evaluate modular arithmetic reasoning across both single-step and multi-step tasks. The datasets are designed to provide explicit numerical structure, balanced supervision, and controlled symbolic complexity, allowing systematic analysis of how different model components handle reasoning, generalization, and modular decomposition.

---
### **4.2.1 Numerical Range Definition**
To ensure semantically meaningful supervision, all arithmetic outputs are mapped into five interpretable numerical intervals:
- **R1:** 2–20  
- **R2:** 21–40  
- **R3:** 41–60  
- **R4:** 61–80  
- **R5:** 81–100  

These intervals function as symbolic categories that partition the numerical domain, enabling experts to specialize over clearly defined knowledge regions.

---
### **4.2.2 Task 1 Dataset: Single-Step Addition (80,000 samples)**
Task 1 evaluates reasoning over explicit, atomic arithmetic relations. Each sample contains:
- Two operands sampled uniformly from 2–50
- A natural-language addition query (e.g., *“What is 17 plus 23?”*)
- A symbolic interval label corresponding to the final result

The dataset is balanced across all five numerical intervals, providing uniform supervision for specialization.

---
### **4.2.3 Task 2 Dataset: Multi-Step Mixed Arithmetic (80,000 samples)**
Task 2 introduces structured symbolic reasoning. Expressions include 2–4 arithmetic operations involving both addition and subtraction.

Examples:
- “18 minus 9 plus 46”
- “29 minus 11 minus 9 minus 7 plus 15”
- “18 plus 7 minus 3”

#### **Expression Length Distribution**
To regulate symbolic complexity:
- **50%** two-step
- **30%** three-step
- **20%** four-step

Operands are sampled independently from 2–50, and expressions are resampled if the final result falls outside 2–100. Addition and subtraction appear with equal probability.

This dataset tests whether the system can integrate multi-step symbolic structure into coherent numerical reasoning.

---
### **4.2.4 Linguistic Template Diversity**
To separate linguistic variability from reasoning difficulty, multiple paraphrased templates are used, such as:
- “Compute 12 minus 5, then add 9.”
- “If you subtract 5 from 12 and add 9, what do you get?”

This encourages the model to acquire underlying reasoning patterns rather than memorizing surface forms.

---
### **4.2.5 Deduplication and Consistency Enforcement**
All datasets undergo a structured cleaning pipeline:
1. **Operational deduplication** — expressions with identical symbolic structures are removed.
2. **Range consistency checking** — final results must map correctly into R1–R5.
3. **Split isolation** — training, validation, and test sets share no overlapping symbolic forms.

These steps ensure that evaluation reflects reasoning generalization rather than pattern memorization.

---
### **4.2.6 Expert Datasets for Modular Execution (Addition & Subtraction Experts)**
Modular execution relies on two specialized arithmetic experts:
- **Addition expert:** 20,000 single-step addition samples
- **Subtraction expert:** 20,000 single-step subtraction samples

Samples take the form:
```
a + b
```
or
```
a – b
```
where both operands are in 2–50, and outputs are mapped to R1–R5.

These datasets represent **atomic symbolic knowledge units**, supporting explicit numerical execution in the modular pipeline.

---
### **4.2.7 Step Decomposer Dataset (80,000 structured samples)**
To support symbolic–neural hybrid reasoning in Section 4.8, an additional dataset provides explicit computational programs paired with natural-language expressions.

A typical record consists of:
```
expr: "33 plus 23 minus 36"
answer: 20
label: 0
num_steps: 2
program:
    Step 1: 33 + 23 = x1
    Step 2: x1 - 36 = x2
```
Each sample encodes:
- the linguistic form,
- the exact computation result,
- the symbolic interval label,
- the number of operations,
- and a fully explicit reasoning program.

#### **Step Count Distribution**
To match symbolic complexity seen in Task 2:
- **20%** two-step
- **40%** three-step
- **40%** four-step

This distribution exposes the decomposer to longer reasoning chains, supporting more reliable symbolic program generation.

#### **Purpose**
The dataset enables the step decomposer to:
- infer symbolic operator ordering,
- generate interpretable intermediate variables,
- output executable reasoning programs consumed by modular experts.

This facilitates a **hybrid symbolic–neural reasoning pipeline**, where symbolic structure directs expert-based numerical execution.

---
## **4.2 Summary**
Together, these datasets:
- provide balanced numerical supervision,
- incorporate linguistic variation,
- control symbolic reasoning complexity,
- enforce clean experimental splits,
- and support both end-to-end and symbolic–neural hybrid reasoning.

This design enables RED to be evaluated under settings that emphasize modularity, interpretability, and explicit reasoning — core principles of knowledge-based systems.

