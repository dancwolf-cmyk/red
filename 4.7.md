## 4.7 Results on Task 2: Mixed Arithmetic

Task 2 extends single-operation addition to mixed arithmetic, including
addition, subtraction, and multi-step expressions with 2–4 operations
(e.g., “12 minus 5 plus 9”). The task is more challenging than Task 1 due to
combinatorial operator patterns and overlapping numerical ranges.

A large portion of the dataset (approximately 80%) consists of
single-operation expressions, which limits the achievable end-to-end
performance ceiling. True multi-step reasoning is therefore analyzed
separately in Section 4.8.

---

### 4.7.1 Overall Performance

| Method                         | Accuracy   | Macro-F1   |
|--------------------------------|------------|------------|
| Global Head                    | 0.3542     | 0.3536     |
| Uniform Averaging              | 0.0105     | 0.0024     |
| MoE Gating                     | 0.5338     | 0.5435     |
| **RED (Ours)**                 | **0.7583** | **0.7537** |

**Interpretation**

- The strengthened base head improves over single-operation tasks but still
  struggles with symbolic multi-step patterns.
- Uniform averaging collapses entirely, confirming that unstructured fusion is
  ineffective.
- Classical MoE achieves moderate gains but is limited by routing ambiguity.
- **RED achieves the strongest performance**, outperforming MoE by an absolute
  margin of $+38\%$ in accuracy.

Because the dataset is dominated by single-operation expressions, end-to-end
accuracy saturates around $0.75$–$0.76$.

---

### 4.7.2 Accuracy by Numerical Range

The task ensures that output values are approximately uniformly distributed
across five numerical ranges:

| Method       | 2–20     | 21–40    | 41–60    | 61–80    | 81–100   |
|--------------|----------|----------|----------|----------|----------|
| Global Head  | 0.5639   | 0.3628   | 0.3389   | 0.2618   | 0.2539   |
| Uniform Avg  | 0.0016   | 0.0000   | 0.0000   | 0.0023   | 0.0025   |
| MoE Gating   | 0.3590   | 0.5537   | 0.5871   | 0.5908   | 0.6074   |
| **RED**      | **0.7879** | **0.8110** | **0.7347** | **0.7269** | **0.7536** |

**Interpretation**

- Base performance declines monotonically across ranges as numerical magnitude
  increases.
- MoE displays instability—strong on some ranges, weaker on others—reflecting
  inconsistent routing behavior.
- **RED maintains strong, consistent performance across all ranges**, enabled by
  stronger expert specialization and residual routing.

---

### 4.7.3 Router Behavior (RED)

| Split       | Accuracy   | Macro-F1  |
|-------------|------------|-----------|
| Train       | 0.8376     | 0.8337    |
| Validation  | 0.7598     | 0.7551    |
| Test        | 0.7583     | 0.7537    |
| **All**     | **0.8219** | **0.8180** |

Routing becomes substantially harder in multi-operator expressions. Despite
this, RED maintains approximately 70–80% routing accuracy. The residual fusion
mechanism mitigates the impact of routing errors, providing stability that
classical MoE cannot achieve.

---

### 4.7.4 Generalization Across Operator Patterns

| Input Type        | Global Head | MoE        | **RED**     |
|-------------------|-------------|------------|--------------|
| addition-only     | medium      | medium     | **high**     |
| subtraction-only  | low–medium  | medium     | **high**     |
| multi-step        | low         | low–medium | **high**     |
| boundary cases    | low         | unstable   | **stable**   |

RED generalizes effectively across operator types and expression structures,
demonstrating robustness that other architectures cannot match.

---

### 4.7.5 Summary of Task 2 Findings

1. Strengthening the base and expert networks helps, but end-to-end symbolic
   multi-step reasoning remains inherently difficult.
2. **RED substantially outperforms both the upgraded base and classical MoE**,
   achieving stable range-specialized performance.
3. Routing is challenging, yet the residual routing strategy of RED ensures
   resilience to misrouting.
4. Task 2 performance is limited by its dataset composition; therefore,
   Section 4.8 evaluates *true* multi-step reasoning using a step decomposer,
   revealing the actual capacity of modular arithmetic models.

