## 4.9 Ablation Studies

This section examines the contribution of major architectural components in
RED across **Task 1 (pure addition)** and **Task 2 (mixed arithmetic)**.
The ablation results reflect the finalized system described in Sections 4.5–4.8.

---

## 4.9.1 Effect of Removing the Residual Path

This study replaces RED’s residual routing with classical softmax MoE gating
while keeping all experts and the backbone unchanged.

| Model Variant | Task 1 Acc | Task 2 Acc |
|--------------|------------|------------|
| MoE (no residual) | 0.8576 | 0.5338 |
| **RED (full model)** | **0.9725** | **0.7583** |

**Findings**

- Removing the residual path results in a substantial accuracy drop
  (–11.5% on Task 1 and –22.5% on Task 2).
- Residual routing stabilizes expert usage and reduces routing errors,
  especially in multi-step arithmetic.
- Classical MoE exhibits error accumulation and inconsistent specialization.

**Conclusion**  
The residual path is essential for stable and accurate modular reasoning.

---

## 4.9.2 Number of Experts

RED is evaluated under two representative expert configurations:

- **Five-expert configuration** for Task 1, each specializing in one output range ($R_1$–$R_5$).  
- **Two-expert configuration** for Task 2 with step-wise decomposition
  (one addition expert, one subtraction expert).

Despite differences in expression complexity and number of steps,
both configurations achieve near-perfect execution accuracy ($\approx 0.98$–$0.99$)
once the computation structure is provided by the step decomposer.

**Observations**

1. RED does not require a large expert pool; two experts suffice for multi-step arithmetic.
2. Structural inference, not expert capacity, is the primary performance bottleneck.
3. Expert count is mainly an engineering choice (e.g., parallel training across machines)
   rather than a critical accuracy-determining factor.

---

## 4.9.3 Bottleneck Size in Residual Adapters

Residual adapters compress expert contributions using a low-rank projection.
We vary the bottleneck ratio $r$.

| Bottleneck Ratio $r$ | Parameter Size | Task 1 Acc | Task 2 Acc |
|----------------------|----------------|------------|------------|
| $r = d/2$ (medium) | medium | 0.969 | 0.750 |
| $r = d/4$ (low) | low | 0.965 | 0.744 |
| $r = d/8$ (very low) | very low | 0.951 | 0.728 |

**Findings**

- Accuracy decreases gradually as $r$ decreases.
- Even with very low rank, RED remains robust.
- The results confirm the efficiency and stability of low-rank residual adapters.

---

## 4.9.4 Comparison with Execution-Based Baselines

To summarize architectural implications without repeating Section 4.8:

| Model | Avg. Acc (2–4 Step) | Behavior |
|-------|----------------------|----------|
| AllFix | 0.3216 | No routing; fails to specialize |
| MoE | 0.8391 | Unstable routing; moderate performance |
| **RED** | **0.9920 (execution)** / **0.7583 (end-to-end)** | Stable routing and specialization |

**Interpretation**

- Routing is the dominant factor underlying multi-step performance.
- RED’s architectural components—residual routing + expert specialization—
  systematically improve over MoE.
- Execution upper bound (0.99+) demonstrates that architectural choices,
  not expert capacity or dataset size, explain RED’s superiority.

---

## 4.9.5 Summary of Ablation Insights

1. **Residual routing is critical**; without it, RED collapses to unstable MoE behavior.
2. **Expert granularity affects performance**, but even small expert sets achieve near-perfect execution when structure is provided.
3. **Low-rank adapters are sufficient**, offering a favorable tradeoff between efficiency and accuracy.
4. **Architectural decisions, not data volume**, drive RED’s improvements over MoE.
5. **Ablation complements Section 4.8**:  
   - Section 4.9 identifies *which architectural components matter most*.  
   - Section 4.8 demonstrates *the theoretical execution ceiling when structure is known*.  

