## 4.5 Training Setup

This section outlines the finalized training configuration used across all
experiments in this study, including Tasks 1–2 and the step-decomposition
evaluation in Section 4.8. The system follows a modular architecture in which
the backbone, experts, routers, and the step decomposer are trained
independently and later integrated into a unified reasoning framework.

---

### 4.5.1 Backbone Configuration

A shared pretrained backbone provides consistent representations for all
downstream modules.

- **Backbone:** Qwen3-0.6B  
- **Trainable layers:** Only the top two transformer layers were unfrozen;
  the remaining layers remained fixed.  
- **Hardware:** RTX 3060 Ti (FP32)  
- **Pooling:** Mean pooling over the final hidden states.

This configuration preserves stable embeddings while allowing limited task
adaptation at the top layers.

---

### 4.5.2 Expert Training

Experts were trained independently on disjoint data partitions corresponding to
their respective numerical subdomains.

- **Addition expert:**  
  20k samples, trained on GTX 1650 Ti (FP32)

- **Subtraction expert:**  
  20k samples, trained on GTX 1650 Ti (FP32)

- **Range experts (Tasks 1–2):**  
  Full 80k dataset, trained on RTX 3060 Ti (FP32)

**Shared hyperparameters**

| Item | Value |
|------|-------|
| Optimizer | AdamW |
| Learning rate | $1 \times 10^{-4}$ |
| Batch size | 16 (virtual batch size 64) |
| Max epochs | 60 |
| Early stopping | patience = 3 |

Arithmetic experts converge quickly due to limited operational complexity,
whereas numerical-range experts benefit from longer training.

---

### 4.5.3 Router Training

Routers determine which expert processes each input based on the backbone
embedding.

- **Hardware:** Three RTX 3050 GPUs (BF16)  
- **Architecture:** Two-layer MLP with hidden size 256  
- **Output:** Five numerical ranges  
- **Router variants:**  
  - RED (residual expert decomposition)  
  - Softmax-based MoE router  
  - Uniform averaging baseline

All routers receive identical backbone features, enabling direct comparison of
routing strategies.

---

### 4.5.4 Step-Decomposition Model

A step decomposer generates symbolic multi-step computation programs from raw
arithmetic expressions:

$$
\begin{aligned}
x_1 &= a \pm b, \\
x_2 &= x_1 \pm c, \\
&\dots
\end{aligned}
$$

The decomposer predicts only the structure of operations— not intermediate
numerical results—ensuring full compatibility with the expert modules.

- **Hardware:** Three RTX 3050 GPUs (BF16)  
- **Input:** Raw arithmetic expression  
- **Output:** Symbolic step sequence using variables $x_1, x_2, \ldots$

This model is trained independently from both experts and routers.

---

### 4.5.5 Modular Integration

After independent training, modules are assembled into a unified execution
pipeline:

1. The **backbone** encodes the expression.  
2. The **step decomposer** produces the symbolic computation steps.  
3. The **router** selects the expert for each step.  
4. The **expert** executes the corresponding operation.

A key advantage of this framework is that components trained on different GPUs,
with different precisions and dataset sizes, can be integrated seamlessly
**without joint finetuning**.

---

### 4.5.6 Dataset Usage

All tasks use an 80k-sample dataset divided into:

- 64k training  
- 8k validation  
- 8k test  

Only 10% of the training split is used for backbone finetuning to prevent
over-specialization. Experts and routers, however, train on their full relevant
datasets.

---

### 4.5.7 Summary

The finalized training pipeline reflects the system’s modular design:

- experts specialize in local numerical domains,  
- routers integrate their contributions globally,  
- the step decomposer structures multi-step reasoning, and  
- all components remain interoperable without global retraining.

This configuration is used consistently across the experiments reported in
Sections 4.6–4.8.
