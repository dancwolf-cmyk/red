## 4.12 Summary

This section has presented a comprehensive empirical evaluation of the Residual
Expert Decomposition (RED) framework across controlled arithmetic tasks. The
results demonstrate that RED enables reliable modular reasoning through the
coordinated interaction of a shared backbone, specialized experts, and a
lightweight residual router.

---

### Key Findings

1. **Stable and interpretable expert specialization**  
   Experts trained on distinct numerical intervals exhibit predictable behavior,
   enabling clear modular structure.

2. **Robust global recomposition via routing**  
   The router consistently selects appropriate experts, and the additive
   residual mechanism preserves model stability even in the presence of routing
   errors.

3. **Residual integration enhances consistency**  
   Residual signals refine expert outputs smoothly, avoiding the volatility
   typically observed in classical MoE architectures.

4. **Generalization across operators and tasks**  
   Although experts are trained only on addition, RED generalizes effectively to
   subtraction and multi-step expressions, highlighting its compositional
   robustness.

---

### Insight from Section 4.8

When supplied with a correct step decomposition, RED achieves **near-perfect
accuracy (>0.99)** on multi-step arithmetic. This indicates that:

- **expert execution is highly reliable**, and  
- **errors in end-to-end RED arise primarily from structural inference**, not
  from computational limitations of the expert modules.

---

### Overall Conclusion of Section 4

Together, the results show that RED is a **lightweight, interpretable, and
computationally efficient modular reasoning framework**. It preserves the
benefits of expert specialization while providing stable and coherent global
predictions. The combination of accuracy, robustness, and efficiency positions
residual routing as a strong alternative to both uniform ensembling and
classical Mixture-of-Experts.
