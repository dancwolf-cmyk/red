## 4.11 Case Study: Modular Reasoning Behavior of RED

This section provides qualitative analyses illustrating how RED performs
end-to-end modular reasoning. Unlike the execution experiments in Section 4.8,
which rely on externally provided step decomposition, the examples here reflect
the behavior of the full model when it must infer structure implicitly.

Each case reports router weights, dominant expert contributions, and RED’s final
output. Architectural details are omitted, as they have been presented earlier.

---

### Example 1 — High-Range Addition

**Query:** “What is 47 plus 38?”  
**Ground truth:** 85 (range $R_5$)

**Router weights**
\[
\alpha = [0.02,\, 0.04,\, 0.14,\, 0.28,\, 0.52]
\]

**Behavior**
- Expert 5 receives the largest weight, consistent with the output range.
- Experts 3 and 4 provide stabilizing residual contributions.
- Final prediction: **85**.

**Insight:** RED’s residual mixing supports stable high-range predictions.

---

### Example 2 — Mid-Range Addition

**Query:** “What is 32 plus 19?”  
**Ground truth:** 51 (range $R_3$)

**Router weights**
\[
\alpha = [0.03,\, 0.06,\, 0.67,\, 0.16,\, 0.08]
\]

**Behavior**
- Expert 3 dominates, matching the target range.
- Adjacent experts contribute corrective residual signals.
- Final prediction: **51**.

**Insight:** RED transitions smoothly across neighboring ranges, avoiding the
discontinuities observed in classical MoE.

---

### Example 3 — Mixed Arithmetic

**Query:** “What is 18 plus 7 minus 3?”  
**Ground truth:** 22 (range $R_2$)

**Router weights**
\[
\alpha = [0.10,\, 0.62,\, 0.18,\, 0.06,\, 0.04]
\]

**Behavior**
- Expert 2 receives the primary weight.
- Experts 1 and 3 add small corrective contributions.
- Final prediction: **22**, despite the fact that experts were trained only on
  single-operation addition.

**Insight:** Residual recomposition enables generalization beyond the specific
operator types seen during expert training.

---

### Example 4 — Boundary Case

**Query:** “What is 40 plus 1?”  
**Ground truth:** 41 (boundary between $R_2$ and $R_3$)

**Router weights**
\[
\alpha = [0.05,\, 0.32,\, 0.46,\, 0.12,\, 0.05]
\]

**Behavior**
- Experts 2 and 3 both contribute substantially.
- RED outputs **41**, maintaining consistency across a range boundary.

**Insight:** Residual routing prevents discontinuities and supports accurate
predictions near numerical boundaries.

---

### Summary of Case Observations

1. **Routing aligns with numerical structure**, assigning highest weights to the
   appropriate experts.
2. **Residual pathways refine predictions**, especially near boundaries.
3. **Generalization to mixed operations** emerges despite experts being trained
   only on addition.
4. **Modular contributions are interpretable**, with predictable, stable expert
   interactions.

These qualitative findings complement the quantitative results presented in
Sections 4.6–4.9.

