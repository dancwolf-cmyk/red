## 4.4 Our Method: Residual Expert Decomposition (RED)

Residual Expert Decomposition (RED) is a modular architecture designed to integrate
multiple independently trained numerical experts through a shared semantic backbone
and a routing mechanism that assigns context-dependent expert contributions. Unlike
classical Mixture-of-Experts (MoE) models, which combine expert logits directly,
RED performs **residual integration**, allowing expert refinements to remain stable,
numerically grounded, and interpretable.

RED consists of three components:

1. a shared backbone encoder,
2. residual expert modules specializing in numerical substructures, and
3. a router that determines expert participation.

The following subsections describe the core components of RED.

---

### 4.4.1 Residual Expert Structure

Each expert functions as a **residual adapter** that refines—rather than replaces—
the backbone representation. Let the backbone produce the embedding

$$
h_{\text{base}}(x).
$$

Expert $E_i$ generates a residual update:

$$
\Delta h_i(x) = \phi_i\bigl(h_{\text{base}}(x)\bigr),
$$

where $\phi_i$ is a low-rank transformation.

This design ensures that:

- experts specialize in range- or operation-specific refinements,
- the global semantic representation is preserved, and
- expert modules remain independent and easily composable.

Experts do not output predictions directly; they only contribute representational updates.

---

### 4.4.2 Router for Expert Mixing

A routing network assigns weights to experts based on the backbone embedding:

$$
\boldsymbol{\alpha}(x) =
\operatorname{softmax}\bigl(R(h_{\text{base}}(x))\bigr).
$$

The router is trained after experts have been fixed, ensuring that experts remain
independent modules rather than co-adapted components.  
The routing weights reflect the model’s assessment of which numerical specialization
is most relevant for the input query.

---

### 4.4.3 Residual Integration

The final representation is obtained by combining the backbone embedding with
expert refinements:

$$
\hat{h}(x)
= h_{\text{base}}(x)
+ \sum_{i=1}^{5} \alpha_i(x)\, \Delta h_i(x).
$$

A shared classifier converts $\hat{h}(x)$ into output logits.

This construction provides several favorable properties:

- **Stability:** the backbone acts as an anchor, and residuals produce bounded updates.
- **Graceful routing error tolerance:** small inaccuracies in routing lead to proportionally small output perturbations.
- **Cross-expert cooperation:** adjacent experts contribute smoothly near numerical boundaries.

---

### 4.4.4 Benefits of Residual Integration

The residual formulation yields structural advantages not available in classical MoE frameworks:

**(1) Stable representational grounding**  
The backbone provides a consistent semantic base that prevents experts from drifting
or interfering with each other.

**(2) Robustness to imperfect routing**  
Because experts modify the representation additively, incorrect routing weights
cannot dramatically distort predictions.

**(3) Modular extensibility**  
Experts may be trained, replaced, or extended in isolation without requiring
retraining of the backbone or other experts.

---

### 4.4.5 Summary

RED enables modular numerical reasoning by combining:

- a shared semantic backbone,
- independently trained residual experts, and
- a routing mechanism that allocates expert contributions.

This structure provides accurate global prediction, stable numerical behavior across
ranges, and interpretable expert specialization, while mitigating the fragility
commonly observed in traditional gating-based MoE architectures.
