## 4.10 Computational Complexity

This section analyzes the computational and memory complexity of four
architectures—Global Head, Uniform Averaging, classical MoE, and RED—under a
shared Qwen-0.6B backbone. Differences arise solely from the modular
components; architectural details are provided in Section 4.4.

---

### 4.10.1 Parameter Cost

Let $d$ denote the hidden dimension, $C$ the number of output classes, and
$K=5$ the number of experts. RED employs a low-rank residual bottleneck of
dimension $r \ll d$.

| Method | Parameter Cost (beyond backbone) | Notes |
|--------|----------------------------------|-------|
| Global Head | $O(dC)$ | single classifier |
| Uniform Avg | $O(K d C)$ | $K$ independent heads |
| MoE (no residual) | $O(K d C) + O(K d)$ | full-rank experts + router |
| **RED (Ours)** | $O(K (dr + rC)) + O(K d)$ | low-rank experts + router |

**Interpretation**

Because $r \ll d$, RED reduces parameter count by **1–2 orders of magnitude**
compared with full-rank MoE experts, making it the most parameter-efficient
multi-expert design.

---

### 4.10.2 Inference FLOPs

All architectures share the same backbone forward pass; thus, only additional
FLOPs are compared.

| Method | Extra FLOPs | Explanation |
|--------|-------------|-------------|
| Global Head | negligible | single linear projection |
| Uniform Avg | very low | $K$ classifiers |
| MoE (no residual) | low | $K$ full-rank heads + router |
| **RED (Ours)** | low–medium | $K$ low-rank projections + router |

**Interpretation**

RED’s overhead stems mainly from low-rank projections, which are significantly
cheaper than MoE’s full-rank heads. Router computation contributes less than
0.5% of backbone FLOPs.

---

### 4.10.3 Memory Footprint

During training and inference:

- Only the top two backbone layers are unfrozen.
- Expert modules remain lightweight even under multi-expert settings.
- The router operates solely on pooled embeddings.

**Peak VRAM usage** on an RTX 3060 Ti (8 GB): **< 6.5 GB**, with no requirement
for tensor parallelism or activation checkpointing.

---

### 4.10.4 Summary

The complexity analysis demonstrates that:

1. **RED achieves the best accuracy–efficiency tradeoff** among all multi-expert models.  
2. **Low-rank residual experts** significantly reduce parameter count relative to MoE.  
3. **Inference overhead is small**, maintaining efficiency while improving stability.  
4. **Memory use remains modest**, enabling deployment on commodity GPUs.

Overall, RED is an efficient and scalable alternative to classical MoE,
delivering strong performance while maintaining low computational cost.
